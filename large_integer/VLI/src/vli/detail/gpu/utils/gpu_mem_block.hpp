/*
*Very Large Integer Library, License - Version 1.0 - May 3rd, 2012
*
*Timothee Ewart - University of Geneva, 
*Andreas Hehn - Swiss Federal Institute of technology Zurich.
*Maxim Milakov â€“ NVIDIA
*
*Permission is hereby granted, free of charge, to any person or organization
*obtaining a copy of the software and accompanying documentation covered by
*this license (the "Software") to use, reproduce, display, distribute,
*execute, and transmit the Software, and to prepare derivative works of the
*Software, and to permit third-parties to whom the Software is furnished to
*do so, all subject to the following:
*
*The copyright notices in the Software and this entire statement, including
*the above license grant, this restriction and the following disclaimer,
*must be included in all copies of the Software, in whole or in part, and
*all derivative works of the Software, unless such copies or derivative
*works are solely in the form of machine-executable object code generated by
*a source language processor.
*
*THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
*IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
*FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
*SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
*FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
*ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
*DEALINGS IN THE SOFTWARE.
*/
// to remove when size ok
#include <assert.h>

namespace vli{
namespace detail {

    template <typename BaseInt>
    gpu_memblock<BaseInt>::gpu_memblock()
    : block_size_(0), V1Data_(0), V2Data_(0), VinterData_(0), PoutData_(0) {
    }

    template <typename BaseInt>
    gpu_memblock<BaseInt>::~gpu_memblock() {
        if (V1Data_ != 0 )
            gpu::cu_check_error(cudaFree((void*)this->V1Data_),__LINE__);
        if (V2Data_ != 0 )
            gpu::cu_check_error(cudaFree((void*)this->V2Data_),__LINE__);
        if(VinterData_ != 0)
            gpu::cu_check_error(cudaFree((void*)this->VinterData_),__LINE__);
        if(PoutData_ != 0)
            gpu::cu_check_error(cudaFree((void*)this->PoutData_),__LINE__);
    }

    template <typename BaseInt>
    std::size_t gpu_memblock<BaseInt>::GetBlockSize() const {
        return block_size_;
    }; 

    template <class BaseInt, std::size_t size, class MaxOrder ,class Var0, class Var1, class Var2, class Var3>
    struct resize_helper{
    };
    // A free function, syntax light compared nested into the class
    // max order each specialization 
    template <class BaseInt, std::size_t size,  int Order,class Var0, class Var1, class Var2, class Var3>
    struct resize_helper<BaseInt, size, max_order_each<Order>, Var0, Var1, Var2, Var3>{
        static void resize(gpu_memblock<BaseInt> const* pgm,  std::size_t vector_size){

        std::size_t req_size = vector_size * size * stride<Var0,Order>::value * stride<Var1,Order>::value * stride<Var2,Order>::value * stride<Var3,Order>::value;

        if( req_size > pgm->GetBlockSize() ) {
            if(pgm->V1Data_ != 0 )
                gpu::cu_check_error(cudaFree((void*)pgm->V1Data_),__LINE__);
            if(pgm->V2Data_ != 0 )
                gpu::cu_check_error(cudaFree((void*)pgm->V2Data_),__LINE__);
            if(pgm->VinterData_ != 0)
                gpu::cu_check_error(cudaFree((void*)pgm->VinterData_),__LINE__);
            if(pgm->PoutData_ != 0)
                gpu::cu_check_error(cudaFree((void*)pgm->PoutData_),__LINE__);
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->V1Data_), req_size*sizeof(BaseInt)),__LINE__); //input 1
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->V2Data_), req_size*sizeof(BaseInt)),__LINE__); //input 2
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->VinterData_), vector_size *  2*size * result_stride<Var0, Order>::value * result_stride<Var1, Order>::value * result_stride<Var2, Order>::value * result_stride<Var3, Order>::value * sizeof(BaseInt)),__LINE__); 
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->PoutData_),                  2*size * result_stride<Var0, Order>::value * result_stride<Var1, Order>::value * result_stride<Var2, Order>::value * result_stride<Var3, Order>::value * sizeof(BaseInt)),__LINE__);
        } // end if

        } // end function
    }; // end struct

    // max order combined specialization 
    template <class BaseInt, std::size_t size, int Order,class Var0, class Var1, class Var2, class Var3>
    struct resize_helper<BaseInt, size, max_order_combined<Order>, Var0, Var1, Var2, Var3>{
        static void resize(gpu_memblock<BaseInt> const* pgm,  std::size_t vector_size){

        std::size_t req_size = vector_size * size * vli::detail::max_order_combined_helpers::size<num_of_variables_helper<Var0,Var1,Var2,Var3>::value+1, Order>::value;

        if( req_size > pgm->GetBlockSize() ) {
            if(pgm->V1Data_ != 0 )
                gpu::cu_check_error(cudaFree((void*)pgm->V1Data_),__LINE__);
            if(pgm->V2Data_ != 0 )
                gpu::cu_check_error(cudaFree((void*)pgm->V2Data_),__LINE__);
            if(pgm->VinterData_ != 0)
                gpu::cu_check_error(cudaFree((void*)pgm->VinterData_),__LINE__);
            if(pgm->PoutData_ != 0)
                gpu::cu_check_error(cudaFree((void*)pgm->PoutData_),__LINE__);

            gpu::cu_check_error(cudaMalloc((void**)&(pgm->V1Data_), req_size*sizeof(BaseInt)),__LINE__); //input 1
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->V2Data_), req_size*sizeof(BaseInt)),__LINE__); //input 2
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->VinterData_), vector_size *  2*size * vli::detail::max_order_combined_helpers::size<num_of_variables_helper<Var0,Var1,Var2,Var3>::value+1, 2*Order>::value*sizeof(BaseInt)),__LINE__); 
            gpu::cu_check_error(cudaMalloc((void**)&(pgm->PoutData_),                  2*size * vli::detail::max_order_combined_helpers::size<num_of_variables_helper<Var0,Var1,Var2,Var3>::value+1, 2*Order>::value*sizeof(BaseInt)),__LINE__);

            } // end if
        } // end fonction
    }; //end struct


    template <class BaseInt, std::size_t Size, class MaxOrder ,class Var0, class Var1, class Var2, class Var3>
    struct memory_transfer_helper;
    
    // max order each specialization 
    template <class BaseInt, std::size_t Size, int Order,class Var0, class Var1, class Var2, class Var3>
    struct memory_transfer_helper<BaseInt, Size, max_order_each<Order>, Var0, Var1, Var2, Var3>{
         static void transfer_up(gpu_memblock<BaseInt> const* pgm, BaseInt const* pData1, BaseInt const* pData2,  std::size_t VectorSize){
  	    gpu::cu_check_error(cudaMemcpyAsync((void*)pgm->V1Data_,(void*)pData1,VectorSize*stride<Var0,Order>::value*stride<Var1,Order>::value*stride<Var2,Order>::value*stride<Var3,Order>::value*Size*sizeof(BaseInt),cudaMemcpyHostToDevice),__LINE__);
  	    gpu::cu_check_error(cudaMemcpyAsync((void*)pgm->V2Data_,(void*)pData2,VectorSize*stride<Var0,Order>::value*stride<Var1,Order>::value*stride<Var2,Order>::value*stride<Var3,Order>::value*Size*sizeof(BaseInt),cudaMemcpyHostToDevice),__LINE__);
         }
    };

    // max order combined  specialization 
    template <class BaseInt, std::size_t Size, int Order,class Var0, class Var1, class Var2, class Var3>
    struct memory_transfer_helper<BaseInt, Size, max_order_combined<Order>, Var0, Var1, Var2, Var3>{
         static void transfer_up(gpu_memblock<BaseInt> const* pgm, BaseInt const* pData1, BaseInt const* pData2,  std::size_t VectorSize){
  	    gpu::cu_check_error(cudaMemcpyAsync((void*)pgm->V1Data_,(void*)pData1,VectorSize*max_order_combined_helpers::size<num_of_variables_helper<Var0,Var1,Var2,Var3>::value+1, Order>::value*Size*sizeof(BaseInt),cudaMemcpyHostToDevice),__LINE__);
  	    gpu::cu_check_error(cudaMemcpyAsync((void*)pgm->V2Data_,(void*)pData2,VectorSize*max_order_combined_helpers::size<num_of_variables_helper<Var0,Var1,Var2,Var3>::value+1, Order>::value*Size*sizeof(BaseInt),cudaMemcpyHostToDevice),__LINE__);
         }
    };

    } // end namespace detail
}// end namespace vli
